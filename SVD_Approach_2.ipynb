{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0vqZu2u/Dvv6X9FjH5Dov",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vipashaaV321/Collaborative-Food-Recipe-Recommendation-System/blob/Vipasha/SVD_Approach_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYyInIP2an1A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IylFMoncawJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrYfUIfdtYyd"
      },
      "outputs": [],
      "source": [
        "Base_path ='/content/drive/MyDrive/Data/RS2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bh6BHfaPtfFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0f0fe59-5146-46f1-8847-2e233e8157c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matrix=np.load(Base_path+\"/user_item_100.npy\")"
      ],
      "metadata": {
        "id": "D5W1OLf7cEM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data=data.pivot(index=\"user_id\",columns='recipe_id',values='rating').fillna(0)\n",
        "U,S,Vt=np.linalg.svd(matrix,full_matrices=False)"
      ],
      "metadata": {
        "id": "pVhsRIbJbcSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rank=np.count_nonzero(S)"
      ],
      "metadata": {
        "id": "T1spYiP0ctLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "S=np.diag(S[:rank])\n",
        "U=U[:,:rank]\n",
        "Vt=Vt[:rank,:]"
      ],
      "metadata": {
        "id": "45_WCMzNc0g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# approximation=np.dot(np.dot(U,S),Vt)+ avg_rating"
      ],
      "metadata": {
        "id": "eldcuUe0c_86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the ratings for all uninteracted items for a user\n",
        "def predict_all(user_id, matrix, item_similarities, k=5):\n",
        "    # get the ratings of the user\n",
        "    user_ratings = matrix[user_id, :]\n",
        "\n",
        "    # get the indices of the uninteracted items\n",
        "    uninteracted_items = np.where(user_ratings == 0)[0]\n",
        "\n",
        "    # predict the rating for each uninteracted item\n",
        "    predictions = np.zeros((len(uninteracted_items),))\n",
        "    for i, item_id in enumerate(uninteracted_items):\n",
        "        # find the k most similar items to the target item\n",
        "        similar_items = np.argsort(-item_similarities[item_id, :])[:k]\n",
        "\n",
        "        # get the ratings of the k most similar items\n",
        "        similar_ratings = matrix[user_id, similar_items]\n",
        "\n",
        "        # compute the weighted average of the ratings\n",
        "        similarity_scores = item_similarities[item_id, similar_items]\n",
        "        prediction = np.dot(similarity_scores, similar_ratings) / np.sum(similarity_scores)\n",
        "\n",
        "        # add the average rating of the item to the prediction\n",
        "        prediction += np.mean(matrix[:, item_id])\n",
        "\n",
        "        # store the prediction\n",
        "        predictions[i] = prediction\n",
        "\n",
        "    # create a dataframe with the item IDs and predicted ratings\n",
        "    predicted_ratings = pd.DataFrame({'item_id': uninteracted_items, 'rating': predictions})\n",
        "\n",
        "    return predicted_ratings"
      ],
      "metadata": {
        "id": "md16SCf6gc5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# convert the input matrix to a sparse matrix\n",
        "sparse_matrix = csr_matrix(matrix)\n",
        "\n",
        "# define the batch size\n",
        "batch_size = 1000\n",
        "\n",
        "# compute the similarity matrix between items incrementally\n",
        "item_similarities = csr_matrix((matrix.shape[1], matrix.shape[1]), dtype=np.float32)\n",
        "for i in range(0, matrix.shape[1], batch_size):\n",
        "    print(f\"Processing items {i}-{i+batch_size-1}\")\n",
        "    batch = sparse_matrix[:, i:i+batch_size]\n",
        "    print(batch.shape)\n",
        "    batch_item_similarities = batch.T.dot(Vt)\n",
        "    item_similarities[i:i+batch_size, :] = batch_item_similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfQJuQ8hj1N8",
        "outputId": "8c46863f-35db-43a2-df07-148379e495ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing items 0-999\n",
            "(1165, 1000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/scipy/sparse/_index.py:125: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Vt.T.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WTmKdjEmxOt",
        "outputId": "1a2c8cc4-4eec-4b63-ee91-ca31eb336c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142485, 1165)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Getting Ram Crashed try to use chuncked data"
      ],
      "metadata": {
        "id": "03MJ1tq3EORq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the batch size\n",
        "batch_size = 1000\n",
        "\n",
        "# compute the similarity matrix between items incrementally\n",
        "item_similarities = np.zeros((matrix.shape[1], matrix.shape[1]))\n",
        "for i in range(0, matrix.shape[1], batch_size):\n",
        "    print(f\"Processing items {i}-{i+batch_size-1}\")\n",
        "    batch = matrix[:, i:i+batch_size]\n",
        "    batch_item_similarities = np.dot(Vt.T[i:i+batch_size, :], Vt.T)\n",
        "    item_similarities[i:i+batch_size, :] = batch_item_similarities"
      ],
      "metadata": {
        "id": "VmS-Z1VqiX_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item_similarities = np.dot(Vt.T, Vt)"
      ],
      "metadata": {
        "id": "_Hly4xg1hzEy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}